{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Notebook for importing and spatialising csv output from HazImp, for QA/QC\n",
    "\n",
    "This notebook searches the specified folder and locates .csv file output (building points and aggregated output) produced by HazImp. This notebook can loop through the output from multiple scenarios.\n",
    "\n",
    "This notebook has been tested with the following folder structure: specified folder -> scenario code folder -> output.csv, specified folder -> scenario code folder -> another identifier folder -> output.csv Therefore .csv output files can be located in the specified folder or a further one or two folders deep.\n",
    "\n",
    "This notebook will use the name of the folder containing the .csv to name the spatialised output.\n",
    "\n",
    "The point and aggregated .csv output are the only .csv that can be located within the specified folder structure and must be of the format ....csv for points and ...agg.csv for aggregated.\n",
    "\n",
    "***\n",
    "This Notebook requires an environment which includes the arcGIS and arcpy modules\n",
    "If you do not have an environment with these modules you can use the default ESRI environment\n",
    "\n",
    "In ArcGIS Pro main menu (or Project menu) select 'Package Manager'\n",
    "Top right next to 'Active Environment' select the cog button\n",
    "clone and rename the default environment, save somewhere appropriate\n",
    "Set this environment as the environment kernel when running this notebook\n",
    "***\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import arcgis\n",
    "from arcgis.gis import GIS\n",
    "gis = GIS()\n",
    "import os\n",
    "import arcpy\n",
    "from arcpy import metadata as md\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Environment and file locations\n",
    "This is the only section that should need to be edited\n",
    "\n",
    "environment is commented out as output locations were used instead. this is so that tables and feature classes could\n",
    "be saved in different locations.\n",
    "if you want all data saved in the same location you can use the environment and remove os.path.join from functions in the loop below\n",
    "\n",
    "the gdbs used for the output locations must already exist, this notebook will not create them.\n",
    "\"\"\"\n",
    "\n",
    "#Update to your working gdb\n",
    "#arcpy.env.workspace = r\"X:\\georisk\\HaRIA_B_Wind\\projects\\acs\\2. DATA\\1. Work Unit Assessment\\NWRA\\impact\\test\\test.gdb\"\n",
    "\n",
    "#Location of specified folder (containing folder structure for all hazimp output)\n",
    "data_loc = r\"X:\\georisk\\HaRIA_B_Wind\\projects\\acs\\2. DATA\\1. Work Unit Assessment\\NWRA\\impact\\AEP\\20231206_run_1\\ACT\"\n",
    "#Location of feature class or shapefile of the aggregate boundaries matching the aggregation used in HazImp\n",
    "agg_boundary_loc = r\"X:\\georisk\\HaRIA_B_Wind\\projects\\acs\\2. DATA\\1. Work Unit Assessment\\NWRA\\NWRA.gdb\\extents\\SA1_2021_AUST\"\n",
    "#Location where the output will be saved (change this to your working gdb)\n",
    "table_out_path = r\"X:\\georisk\\HaRIA_B_Wind\\projects\\acs\\2. DATA\\1. Work Unit Assessment\\NWRA\\impact\\test\\test2.gdb\"\n",
    "feature_out_path = r\"X:\\georisk\\HaRIA_B_Wind\\projects\\acs\\2. DATA\\1. Work Unit Assessment\\NWRA\\impact\\test\\test.gdb\"\n",
    "\n",
    "\"\"\"\n",
    "the time and the spatial reference of the boundaries\n",
    "\"\"\"\n",
    "\n",
    "#Time string gives each table and feature class created a unique name. This is useful to ensure data is not overwritten\n",
    "#A spatial reference is needed when creating the feature classes, for simplicity we use the spatial reference from the boundary layer\n",
    "#The geographic coordinate system is GDA2020\n",
    "timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "spatial_ref = arcpy.Describe(agg_boundary_loc).spatialReference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define metadata\n",
    "\n",
    "#Metadata text can be defined here. {} indicates that a variable will be used in these locations.\n",
    "#Variables are defined in the loop\n",
    "#If you would like to change the variables or add variables you will need to edit the .format(variable1) function in the loop\n",
    "\n",
    "#Aggregated table metadata\n",
    "agg_table_title = '{}' # name defined in the loop\n",
    "agg_table_tags = 'NWRA, ACS, risk, impact'\n",
    "agg_table_summary = 'Raw tabulated {} SA1 aggregated loss values for a recurrence interval of {} years.' # state name and recurrence interval years\n",
    "agg_table_description = 'Raw, tabulated HazImp calculations of average structural loss ratio, total structural loss value, \\\n",
    "and other values for residential buildings in {} exposed to wind speeds with a recurrence interval of {} years, \\\n",
    "aggregated to SA1 code (2021). Raw HazImp data location {}' # state name, recurrence interval years, location of input table in directory\n",
    "agg_table_credits = 'Commonwealth of Australia (Geoscience Australia)'\n",
    "agg_table_accessConstraints = 'Creative Commons Attribution 4.0 International Licence. \\\n",
    "<a href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</a>'\n",
    "\n",
    "#Aggregated feature class metadata\n",
    "agg_feature_title = '{}' # name defined in the loop\n",
    "agg_feature_tags = agg_table_tags\n",
    "agg_feature_summary = '{} SA1 aggregated loss values for a recurrence interval of {} years.' # state name and recurrence interval years\n",
    "agg_feature_description = 'HazImp calculations of average structural loss ratio for residential buildings \\\n",
    "in {} exposed to wind speeds with a recurrence interval of {} years, aggregated to SA1 code (2021). Spatialised \\\n",
    "using raw HazImp output from {}' # state name, recurrence interval years, location of input table in directory\n",
    "agg_feature_credits = agg_table_credits\n",
    "agg_feature_accessConstraints = agg_table_accessConstraints\n",
    "\n",
    "#Points table metadata\n",
    "points_table_title = '{}' # name defined in the loop\n",
    "points_table_tags = agg_table_tags\n",
    "points_table_summary = 'Raw tabulated {} unit level loss values for a recurrence interval of {} years.' # state name and recurrence interval years\n",
    "points_table_description = 'Raw, tabulated HazImp calculations of structural loss ratio and structural loss value for \\\n",
    "individual residential buildings in {} exposed to wind speeds with a recurrence interval of {} years. \\\n",
    "Raw HazImp data location {}' # state name, recurrence interval years, location of input table in directory\n",
    "points_table_credits = agg_table_credits\n",
    "points_table_accessConstraints = agg_table_accessConstraints\n",
    "\n",
    "#Points feature class metadata\n",
    "points_feature_title = '{}' # name defined in the loop\n",
    "points_feature_tags = agg_table_tags\n",
    "points_feature_summary = '{} unit level loss values for a recurrence interval of {} years.' # state name and recurrence interval years\n",
    "points_feature_description = 'HazImp calculations of structural loss ratio for residential buildings \\\n",
    "in {} exposed to wind speeds with a recurrence interval of {} years. Spatialised \\\n",
    "using raw HazImp output from {}' # state name, recurrence interval years, location of input table in directory\n",
    "points_feature_credits = agg_table_credits\n",
    "points_feature_accessConstraints = agg_table_accessConstraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the loop\n",
    "\"\"\"\n",
    "\n",
    "#Walk through all of the files in data location\n",
    "#The data location folder is root\n",
    "#dirs is the next folder down (scenario code folder) if it exists\n",
    "#Files are all of the files in the above folder structure\n",
    "\n",
    "#If using a folder structure which contains a dirs you may need to edit some of the path locations \n",
    "for root, dirs, files in os.walk(data_loc):\n",
    "    for files in files:\n",
    "        #Define come variables which will be used for naming files and in metadata\n",
    "        code = os.path.basename(os.path.normpath(root))\n",
    "        code2 = os.path.basename(os.path.normpath(files))\n",
    "        code2 = code2[:-4]\n",
    "        code3 = code2[2:]\n",
    "        \n",
    "        #Although we search define what to do with 'agg.csv' first\n",
    "        #The loop actually runs the '.csv' files first (I'm not sure why, possible because they come first alphabetcally)\n",
    "\n",
    "        #For the aggregated HazImp output\n",
    "        if files.endswith('agg.csv'):\n",
    "            #Variables for the location of the file in use and for metadata\n",
    "            hazimp_agg_loc = os.path.join(root,files)\n",
    "            code4 = code3[:-4]\n",
    "\n",
    "            #convert csv to table in ArcGIS, create a copy of the aggregate boundaries to use for the join, define names for agg table and feature class\n",
    "            agg_table_name = 'hazimp_' + code + '_' + code2 + '_table_' + timestr\n",
    "            agg_feature_name = 'hazimp_' + code + '_' + code2 + '_SA1_' + timestr\n",
    "            arcpy.conversion.TableToTable(in_rows = hazimp_agg_loc,\n",
    "                                          out_path = table_out_path,\n",
    "                                          out_name = agg_table_name)\n",
    "            arcpy.conversion.ExportFeatures(in_features = agg_boundary_loc,\n",
    "                                            out_features = os.path.join(feature_out_path, agg_feature_name))\n",
    "            \n",
    "            #Add a field to the hazimp agg table and assign the SA1 CODE values \n",
    "            #(SA1 in the hazimp output is a double but it is text in the boundaries feature class, they need to be the same in order for the join to work)\n",
    "            arcpy.management.AddField(in_table = os.path.join(table_out_path, agg_table_name),\n",
    "                                      field_name = 'SA1_CODE21',\n",
    "                                      field_type = 'TEXT')\n",
    "            arcpy.management.CalculateField(in_table = os.path.join(table_out_path, agg_table_name),\n",
    "                                            field = 'SA1_CODE21',\n",
    "                                            expression = '!SA1_CODE!',\n",
    "                                            expression_type = 'PYTHON3')\n",
    "            \n",
    "            #Join the aggregated hazimp output table with the SA1 boundaries\n",
    "            arcpy.management.JoinField(in_data = os.path.join(feature_out_path, agg_feature_name),\n",
    "                                       in_field = 'SA1_CODE21',\n",
    "                                       join_table = os.path.join(table_out_path, agg_table_name),\n",
    "                                       join_field = 'SA1_CODE21')\n",
    "\n",
    "            #The copy of the SA1 boundaries we used is a national dataset but the data we are joining is not national \n",
    "            #Use update cursor to delete rows (SA1s) which have null values\n",
    "            #This will leave only the SA1s in the state which have residential building (and loss) data\n",
    "            with arcpy.da.UpdateCursor(os.path.join(feature_out_path, agg_feature_name), 'FID', 'FID is NULL') as cursor:\n",
    "                for row in cursor:\n",
    "                        cursor.deleteRow()\n",
    "\n",
    "            #Create a metadata file and add predefined text for the agg table\n",
    "            agg_table_md = md.Metadata()\n",
    "            agg_table_md.title = agg_table_title.format(agg_table_name)\n",
    "            agg_table_md.tags = agg_table_tags\n",
    "            agg_table_md.summary = agg_table_summary.format(code, code4)\n",
    "            agg_table_md.description = agg_table_description.format(code, code4, hazimp_agg_loc)\n",
    "            agg_table_md.credits = agg_table_credits\n",
    "            agg_table_md.accessConstraints = agg_table_accessConstraints\n",
    "\n",
    "            #Select the agg table metadata and overwrite it with the predefined metadata\n",
    "            target_md = md.Metadata(os.path.join(table_out_path, agg_table_name))\n",
    "            if not target_md.isReadOnly:\n",
    "                 target_md.copy(agg_table_md)\n",
    "                 target_md.save()\n",
    "            \n",
    "            #Create a metadata file and add predefined text for the agg feature class\n",
    "            agg_feature_md = md.Metadata()\n",
    "            agg_feature_md.title = agg_feature_title.format(agg_feature_name)\n",
    "            agg_feature_md.tags = agg_feature_tags\n",
    "            agg_feature_md.summary = agg_feature_summary.format(code, code4)\n",
    "            agg_feature_md.description = agg_feature_description.format(code, code4, hazimp_agg_loc)\n",
    "            agg_feature_md.credits = agg_feature_credits\n",
    "            agg_feature_md.accessConstraints = agg_feature_accessConstraints\n",
    "\n",
    "            #Select the agg feature class metadata and overwrite it with the predefined metadata\n",
    "            target_md = md.Metadata(os.path.join(feature_out_path, agg_feature_name))\n",
    "            if not target_md.isReadOnly:\n",
    "                 target_md.copy(agg_feature_md)\n",
    "                 target_md.save()\n",
    "\n",
    "        #For point HazImp output\n",
    "        elif files.endswith('.csv'):\n",
    "            #Variable for the location of the file in use\n",
    "            hazimp_points_loc = os.path.join(root,files)\n",
    "\n",
    "            #Convert csv to table in ArcGIS, define name for the points table\n",
    "            points_table_name = 'hazimp_' + code + '_' + code2 + '_points_table_' + timestr\n",
    "            arcpy.conversion.TableToTable(in_rows = hazimp_points_loc,\n",
    "                                          out_path = table_out_path,\n",
    "                                          out_name = points_table_name)\n",
    "            \n",
    "            #Spatialise the hazimp point output using the XY coordinates, define name for the points feature class\n",
    "            points_feature_name = 'hazimp_' + code + '_' + code2 + '_points_' + timestr\n",
    "            arcpy.management.XYTableToPoint(in_table = os.path.join(table_out_path, points_table_name),\n",
    "                                            out_feature_class = os.path.join(feature_out_path, points_feature_name),\n",
    "                                            x_field = 'exposure_longitude',\n",
    "                                            y_field = 'exposure_latitude',\n",
    "                                            coordinate_system = arcpy.SpatialReference(spatial_ref.factoryCode))\n",
    "            \n",
    "            #Create a metadata file and add predefined text for the points table\n",
    "            points_table_md = md.Metadata()\n",
    "            points_table_md.title = points_table_title.format(points_table_name)\n",
    "            points_table_md.tags = points_table_tags\n",
    "            points_table_md.summary = points_table_summary.format(code, code3)\n",
    "            points_table_md.description = points_table_description.format(code, code3, hazimp_points_loc)\n",
    "            points_table_md.credits = points_table_credits\n",
    "            points_table_md.accessConstraints = points_table_accessConstraints\n",
    "\n",
    "            #Select the points table metadata and overwrite it with the predefined metadata\n",
    "            target_md = md.Metadata(os.path.join(table_out_path, points_table_name))\n",
    "            if not target_md.isReadOnly:\n",
    "                 target_md.copy(points_table_md)\n",
    "                 target_md.save()\n",
    "\n",
    "            #Create a metadata file and add predefined text for the points feature class\n",
    "            points_feature_md = md.Metadata()\n",
    "            points_feature_md.title = points_feature_title.format(points_feature_name)\n",
    "            points_feature_md.tags = points_feature_tags\n",
    "            points_feature_md.summary = points_feature_summary.format(code, code3)\n",
    "            points_feature_md.description = points_feature_description.format(code, code3, hazimp_points_loc)\n",
    "            points_feature_md.credits = points_feature_credits\n",
    "            points_feature_md.accessConstraints = points_feature_accessConstraints\n",
    "\n",
    "            #Select the points feature class metadata and overwrite it with the predefined metadata\n",
    "            target_md = md.Metadata(os.path.join(feature_out_path, points_feature_name))\n",
    "            if not target_md.isReadOnly:\n",
    "                 target_md.copy(points_feature_md)\n",
    "                 target_md.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
