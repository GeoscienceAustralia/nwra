{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook calculates average annual loss from the HazImp output of multiple recurrence interval scenarios.\n",
    "\n",
    "It also creates multiple graphs and imports and spatialises the data in ArcGIS Pro.\n",
    "\n",
    "Depending on the formatting, file structure and the naming convention of the HazImp AEP output files, \n",
    "references and variables throughout this notebook will need to be changed.\n",
    "\n",
    "Please review the entire notebook to edit variables and commenting to ensure that the notebook will run as intended.\n",
    "'''\n",
    "\n",
    "import os\n",
    "os.environ[\"CRYPTOGRAPHY_OPENSSL_NO_LEGACY\"]=\"yes\"\n",
    "import arcgis\n",
    "from arcgis.gis import GIS\n",
    "gis = GIS()\n",
    "import arcpy\n",
    "from arcpy import metadata as md\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.integrate import simpson\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join as pjoin\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the attributes which represent the impact (loss) values. These values must be written as they are in the HazImp output attribute tables.\n",
    "LOSSFIELD = \"structural_mean\"\n",
    "LOSSFIELD2 = \"structural_loss_sum\"\n",
    "\n",
    "#Define the location of the input data (HazImp output), the location to save the output tables and graphs, and the ArcGIS output.\n",
    "BASEPATH = r\"X:\\georisk\\HaRIA_B_Wind\\projects\\acs\\2. DATA\\1. Work Unit Assessment\\NWRA\\impact\\AEP\\20231206_run_1\\ACT\"\n",
    "#BASEPATH = r\"X:\\georisk\\HaRIA_B_Wind\\projects\\qfes_swha\\data\\DRAFT DATA STRUCTURES\\1. Work Unit Assessment\\SOUTH EAST QUEENSLAND\\Risk\\risk_pp_baseline\"\n",
    "OUTPATH = r\"X:\\georisk\\HaRIA_B_Wind\\projects\\acs\\2. DATA\\1. Work Unit Assessment\\NWRA\\impact\\AAL\\20231206_run_1\\test\"\n",
    "table_out_path = r\"X:\\georisk\\HaRIA_B_Wind\\projects\\acs\\2. DATA\\1. Work Unit Assessment\\NWRA\\impact\\test\\test.gdb\"\n",
    "feature_out_path = r\"X:\\georisk\\HaRIA_B_Wind\\projects\\acs\\2. DATA\\1. Work Unit Assessment\\NWRA\\impact\\test\\test2.gdb\"\n",
    "\n",
    "#Define the location of the feature class or shapefile of the aggregate boundaries matching the aggregation used in HazImp.\n",
    "agg_boundary_loc = r\"X:\\georisk\\HaRIA_B_Wind\\projects\\acs\\2. DATA\\1. Work Unit Assessment\\NWRA\\NWRA.gdb\\extents\\SA1_2021_AUST\"\n",
    "\n",
    "#The state name variable is the name of the state or scenario being run, this is used to label graphs and name output files.\n",
    "state_name = \"ACT\"\n",
    "\n",
    "#Extracting the recurrence intervals.\n",
    "\n",
    "#The method of doing this may change depending on where the recurrence intervals can be referenced from.\n",
    "\n",
    "#In this example recurrence interval is referenced from the file names in the input path,\n",
    "#any occurrence of '_agg' is removed, the first two characters (RP) are removed, and the last 4 characters are removed (the .filetype).\n",
    "#Duplicates are removed and what is left is a list of recurrence intervals.\n",
    "#Recurrence interval is then converted from a string to an integer and then sorted.\n",
    "ARIS = os.listdir(BASEPATH)\n",
    "ARIS = ([s.replace('_agg', '') for s in ARIS])\n",
    "ARIS = [sub[2: -4] for sub in ARIS]\n",
    "ARIS = list(dict.fromkeys(ARIS))\n",
    "ARIS = list(map(int, ARIS))\n",
    "ARIS = sorted(ARIS)\n",
    "\n",
    "#In some instances, you may need to add a 1 to the beginning of the list of recurrence intervals in order for calculation to be made.\n",
    "#ARIS = [1] + ARIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty dataframe (one for each LOSSFIELD) with SA1 CODE and the recurrence intervals as the columns.\n",
    "lossdf = pd.DataFrame(columns=[\"SA1_CODE\", *ARIS])\n",
    "lossdf2 = pd.DataFrame(columns=[\"SA1_CODE\", *ARIS])\n",
    "\n",
    "#Define the function for calculating aep (probability) from ari (recurrence interval).\n",
    "def probability(ari):\n",
    "    \"\"\"Return an annual probability given a return period\"\"\"\n",
    "    aep = 1.0 - np.exp(-1.0/ari)\n",
    "    return aep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following section of the notebook calculates AAL and creates graphs for a single region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the location of a file which contains all SA1 codes (this can be any aggregated HazImp output file).\n",
    "SA1_CODE_ref = os.path.join(BASEPATH, f\"RP1_agg.csv\")\n",
    "#Define the location of all aggregated HazImp output files using a general format.\n",
    "file_format = os.path.join(BASEPATH, \"RP{}_agg.csv\")\n",
    "\n",
    "#Define a function which calculates AAL stats and saves these to a csv.\n",
    "def AAL_loss_stats(df,state_name,lossfield,outpath):\n",
    "    Max = df['AAL'].max()\n",
    "    Min = df['AAL'].min()\n",
    "    Mean = df['AAL'].mean()\n",
    "    Median = df['AAL'].median()\n",
    "    Std = df['AAL'].std()\n",
    "    loss_stats = pd.DataFrame({\"Dataset\": [f\"{lossfield}_{state_name}\"],\n",
    "                               \"Max\": [Max],\n",
    "                               \"Min\": [Min],\n",
    "                               \"Mean\": [Mean],\n",
    "                               \"Median\": [Median],\n",
    "                               \"Std\": [Std]})\n",
    "    loss_stats.set_index(\"Dataset\", inplace=True)\n",
    "    loss_stats.to_csv(pjoin(outpath,\n",
    "                    f\"{lossfield}_{state_name}_AAL_loss_stats.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculations using Structural loss ratio (LOSSFIELD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set SA1 code as the index for the loss dataframe.\n",
    "firstdf = pd.read_csv(SA1_CODE_ref)\n",
    "lossdf['SA1_CODE'] = firstdf['SA1_CODE']\n",
    "lossdf.set_index('SA1_CODE', inplace=True)\n",
    "\n",
    "#Import all ari output flies and append loss value (LOSSFIELD) to loss dataframe.\n",
    "for ARI in ARIS:\n",
    "    if ARI == 1: continue\n",
    "    tmpdf = pd.read_csv(file_format.format(ARI))\n",
    "    tmpdf.set_index('SA1_CODE', inplace=True)\n",
    "    lossdf = lossdf.join(tmpdf[LOSSFIELD])\n",
    "    lossdf[ARI] = lossdf[LOSSFIELD]\n",
    "    lossdf.drop(LOSSFIELD, axis=1, inplace=True)\n",
    "lossdf[1] = 0\n",
    "\n",
    "#Calculate average annual loss and save .csv. This df includes SA1_CODE, loss value for all ARIS and AAL.\n",
    "aeps = probability(np.array(lossdf.columns.to_list()))\n",
    "lossdf['AAL'] = (lossdf).apply(simpson, axis=1, x=-1*aeps)\n",
    "#When importing a table into ArcGIS Pro the column names cannot begin with a number (even if the number is set as a string).\n",
    "#Add a string to the beginning of the recurrence interval values and define the location of this table for later reference.\n",
    "lossdf.columns = lossdf.columns.map(str)\n",
    "lossdf = lossdf.rename(columns={c: 'RP' + c for c in lossdf.columns if c not in ['SA1_CODE', 'AAL']})\n",
    "lossdf.to_csv(pjoin(OUTPATH, f\"{LOSSFIELD}_{state_name}_SA1.csv\"))\n",
    "ArcGIS_table_loc = pjoin(OUTPATH, f\"{LOSSFIELD}_{state_name}_SA1.csv\")\n",
    "#Save AEP calculations.\n",
    "aeps_save = pd.DataFrame(aeps)\n",
    "aeps_save.to_csv(pjoin(OUTPATH, f\"{LOSSFIELD}_{state_name}_aeps.csv\"))\n",
    "#Calculate and save loss stats.\n",
    "lossdf_stats = AAL_loss_stats(lossdf, state_name, LOSSFIELD, OUTPATH)\n",
    "\n",
    "#Plot histogram of AAL (structural loss ratio) counts.\n",
    "#SA1 agg\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.histplot(lossdf['AAL'], ax=ax)\n",
    "ax.set_yscale('log')\n",
    "plt.xlim(left=0)\n",
    "ax.set_xlabel('AAL')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(f'{state_name} SA1 aggregate')\n",
    "plt.savefig(pjoin(OUTPATH,\n",
    "                f\"{LOSSFIELD}_{state_name}_SA1\"))\n",
    "\n",
    "#Finding mean of structural loss ratio mean to create AAL prob plot.\n",
    "#SA1 agg\n",
    "lossdf = lossdf.drop(['AAL'], axis=1)\n",
    "lossdf = lossdf.mean(axis=0)\n",
    "\n",
    "\n",
    "#Creating AAL prob plot.\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.lineplot(x=lossdf, y=aeps, ax=ax)\n",
    "ax.set_yscale('log')\n",
    "plt.xlim([0,0.55])\n",
    "plt.ylim(bottom=0.0001)\n",
    "ax.set_xlabel('Structural Loss Ratio')\n",
    "ax.set_ylabel('Annual Exceedance Probability')\n",
    "ax.set_title(f'{state_name} SA1 aggregate')\n",
    "ax.fill_between(lossdf, aeps, alpha=0.2)\n",
    "plt.savefig(pjoin(OUTPATH,\n",
    "                f\"{LOSSFIELD}_{state_name}_SA1_probability_loss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculations using Structural loss (AUD) (LOSSFIELD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat the previous steps for other LOSSFIELDS\n",
    "\n",
    "firstdf2 = firstdf\n",
    "lossdf2['SA1_CODE'] = firstdf2['SA1_CODE']\n",
    "lossdf2.set_index('SA1_CODE', inplace=True)\n",
    "\n",
    "for ARI in ARIS:\n",
    "    if ARI == 1: continue\n",
    "    tmpdf2 = pd.read_csv(os.path.join(BASEPATH, f\"RP{ARI:d}_agg.csv\"))\n",
    "    tmpdf2.set_index('SA1_CODE', inplace=True)\n",
    "    lossdf2 = lossdf2.join(tmpdf2[LOSSFIELD2])\n",
    "    lossdf2[ARI] = lossdf2[LOSSFIELD2]\n",
    "    lossdf2.drop(LOSSFIELD2, axis=1, inplace=True)\n",
    "lossdf2[1] = 0\n",
    "\n",
    "aeps = probability(np.array(lossdf2.columns.to_list()))\n",
    "lossdf2['AAL'] = lossdf2.apply(simpson, axis=1, x=-1*aeps)\n",
    "lossdf2.columns = lossdf2.columns.map(str)\n",
    "lossdf2 = lossdf2.rename(columns={c: 'RP' + c for c in lossdf2.columns if c not in ['SA1_CODE', 'AAL']})\n",
    "lossdf2.to_csv(pjoin(OUTPATH, f\"{LOSSFIELD2}_{state_name}_SA1.csv\"))\n",
    "ArcGIS_table_loc2 = pjoin(OUTPATH, f\"{LOSSFIELD2}_{state_name}_SA1.csv\")\n",
    "lossdf2 = lossdf2.div(1000000) #structural loss values are very high, so divide by 1 mill to turn values from $ to million$\n",
    "lossdf2_stats = AAL_loss_stats(lossdf2, state_name, f'{LOSSFIELD2}_(Millions)' , OUTPATH)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.histplot(lossdf2['AAL'], ax=ax)\n",
    "ax.set_yscale('log')\n",
    "plt.xlim(left=0)\n",
    "ax.set_xlabel('AAL (Millions)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(f'{state_name} SA1 aggregate')\n",
    "plt.savefig(pjoin(OUTPATH,\n",
    "                f\"{LOSSFIELD2}_{state_name}_SA1\"))\n",
    "\n",
    "lossdf2 = lossdf2.drop(['AAL'], axis=1)\n",
    "lossdf2 = lossdf2.sum(axis=0)\n",
    "lossdf2 = lossdf2.div(1000)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.lineplot(x=lossdf2, y=aeps, ax=ax)\n",
    "ax.set_yscale('log')\n",
    "plt.xlim(left=0)\n",
    "plt.ylim(bottom=0.0001)\n",
    "ax.set_xlabel('Structural Loss Value (Billions)')\n",
    "ax.set_ylabel('Annual Exceedance Probability')\n",
    "ax.set_title(f'{state_name} SA1 aggregate')\n",
    "ax.fill_between(lossdf2, aeps, alpha=0.2)\n",
    "plt.savefig(pjoin(OUTPATH,\n",
    "                f\"{LOSSFIELD2}_SA1_probability_loss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following functions and examples can be used when you want to calculate AAL and create graphs for smaller areas within the area of interest\n",
    "e.g. calculating AAL by LGA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Define the location of a file which contains all SA1 codes (this can be any aggregated HazImp output file).\n",
    "# SA1_CODE_ref2 = os.path.join(BASEPATH, f\"2\\windspeed_2_yr_agg.csv\")\n",
    "# #Define the location of a file which contains all LGA codes (this can be any unit level HazImp output file).\n",
    "# LGA_CODE_ref = os.path.join(BASEPATH, f\"2\\windspeed_2_yr.csv\")\n",
    "# #Define the location of all aggregated HazImp output files using a general format.\n",
    "# file_format2 = os.path.join(BASEPATH, \"{}\\windspeed_{}_yr_agg.csv\")\n",
    "\n",
    "# #Create a dataframe which contains the names of the LGAs for each SA1.\n",
    "# df = pd.read_csv(LGA_CODE_ref)\n",
    "# df.drop(df.columns.difference(['SA1_CODE','LGA_CODE','LGA_NAME']), axis=1, inplace=True)\n",
    "# df.drop_duplicates(subset=['SA1_CODE'], inplace=True) #(SA1 boundaries and LGA boundaries do not always align, just keep one instance of SA1 code).\n",
    "# df.set_index('SA1_CODE', inplace=True)\n",
    "\n",
    "# #Define the LGAs of Interest and remove data which is not within those LGAs.\n",
    "# LGAs_6 = ['Noosa (S)', 'Sunshine Coast (R)', 'Moreton Bay (R)', 'Brisbane (C)', 'Gold Coast (C)', 'Redland (C)'] #Edit this with the names of interest.\n",
    "# LGAs = df[df['LGA_NAME'].isin(LGAs_6)]\n",
    "# LGAs = LGAs.drop_duplicates(subset=['LGA_NAME'])\n",
    "# LGAs = LGAs.reset_index()\n",
    "# LGAs = LGAs.drop(['SA1_CODE'],axis=1)\n",
    "# LGA_codes = df[df['LGA_NAME'].isin(LGAs_6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Define functions which create plots.\n",
    "\n",
    "# #In all of the following definitions, state_name could be LGA name, this is just used to title the plot and name the output.\n",
    "# #Type can be used if multiple versions of scenarios are run, e.g. retrofit scenarios. This variable is used to create a separate output folder for different runs.\n",
    "# #If it is not needed it can be removed.\n",
    "\n",
    "# #AAL stats, this version of AAL stats is slightly different because of the differences in the folder structure of the input data and the naming convention of the output data used.\n",
    "# def AAL_loss_stats2(df,state_name,lossfield,outpath,type):\n",
    "#     Max = df['AAL'].max()\n",
    "#     Min = df['AAL'].min()\n",
    "#     Mean = df['AAL'].mean()\n",
    "#     Median = df['AAL'].median()\n",
    "#     Std = df['AAL'].std()\n",
    "#     loss_stats = pd.DataFrame({\"Dataset\": [f\"{lossfield}_{state_name}\"],\n",
    "#                                \"Max\": [Max],\n",
    "#                                \"Min\": [Min],\n",
    "#                                \"Mean\": [Mean],\n",
    "#                                \"Median\": [Median],\n",
    "#                                \"Std\": [Std]})\n",
    "#     loss_stats.set_index(\"Dataset\", inplace=True)\n",
    "#     loss_stats.to_csv(pjoin(outpath,type,\n",
    "#                     f\"{lossfield}_{state_name}_loss_stats.csv\"))\n",
    "\n",
    "# #Plotaal_count_ratio plots a histogram of AAL (structural loss ratio) counts \n",
    "# def plotaal_count_ratio(df,state_name,lossfield,outpath,type):\n",
    "#     med_loss = df['AAL'].median()\n",
    "#     fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#     sns.histplot(df['AAL'], ax=ax)\n",
    "#     plt.xlim([0,0.008]) #You might need to change this depending on your data\n",
    "#     ax.set_yscale('log')\n",
    "#     ax.set_xlabel('AAL')\n",
    "#     ax.set_ylabel('Count')\n",
    "#     plt.axvline(med_loss, color='k', linestyle='dashed', linewidth=1.5)\n",
    "#     ax.set_title(state_name)\n",
    "#     plt.savefig(pjoin(outpath, type,\n",
    "#                     f\"{lossfield}_{state_name}_AAL.png\"))\n",
    "\n",
    "# #Plotaal_count_loss plots a histogram of AAL (structural loss AUD) counts \n",
    "# def plotaal_count_loss(df,state_name,lossfield,outpath,type):\n",
    "#     fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#     sns.histplot(df['AAL'], ax=ax)\n",
    "#     plt.xlim(left=0)\n",
    "#     ax.set_yscale('log')\n",
    "#     ax.set_xlabel('AAL (Millions)')\n",
    "#     ax.set_ylabel('Count')\n",
    "#     ax.set_title(state_name)\n",
    "#     plt.savefig(pjoin(outpath, type,\n",
    "#                     f\"{lossfield}_{state_name}_AAL.png\"))\n",
    "          \n",
    "# #Expected loss plot for AAL structural loss ratio\n",
    "# def plotaal_prob_ratio(df,state_name,lossfield,outpath,type):\n",
    "#     fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#     sns.lineplot(x=df, y=aeps, ax=ax)\n",
    "#     ax.set_yscale('log')\n",
    "#     plt.xlim([0,0.55])\n",
    "#     plt.ylim(bottom=0.0001)\n",
    "#     ax.set_xlabel('Structural Loss Ratio')\n",
    "#     ax.set_ylabel('Annual Exceedance Probability')\n",
    "#     ax.set_title(state_name)\n",
    "#     ax.fill_between(df, aeps, alpha=0.2)\n",
    "#     plt.savefig(pjoin(outpath, type,\n",
    "#                     f\"{lossfield}_{state_name}_probability_loss.png\"))\n",
    "\n",
    "# # Expected loss plot for AAL structural loss AUD (change the x label if you are not using billions)\n",
    "# def plotaal_prob_loss(df,state_name,lossfield,outpath,type):\n",
    "#     fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#     sns.lineplot(x=df, y=aeps, ax=ax)\n",
    "#     ax.set_yscale('log')\n",
    "#     plt.xlim([0,95])\n",
    "#     plt.ylim(bottom=0.0001)\n",
    "#     ax.set_xlabel('Structural Loss Value (Billions)')\n",
    "#     ax.set_ylabel('Annual Exceedance Probability')\n",
    "#     ax.fill_between(df, aeps, alpha=0.2)\n",
    "#     ax.set_title(state_name)\n",
    "#     plt.savefig(pjoin(outpath, type,\n",
    "#                     f\"{lossfield}_{state_name}_probability_loss.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Set SA1 code for loss dataframe\n",
    "# firstdf = pd.read_csv(SA1_CODE_ref2)\n",
    "# lossdf['SA1_CODE'] = firstdf['SA1_CODE_']\n",
    "# lossdf.set_index('SA1_CODE', inplace=True)\n",
    "\n",
    "# #Import all ari output flies and append loss value (LOSSFIELD)\n",
    "# for ARI in ARIS:\n",
    "#     if ARI == 1: continue\n",
    "#     tmpdf = pd.read_csv(file_format2.format(ARI, ARI))\n",
    "#     tmpdf.set_index('SA1_CODE_', inplace=True)\n",
    "#     lossdf = lossdf.join(tmpdf[LOSSFIELD])\n",
    "#     lossdf[ARI] = lossdf[LOSSFIELD]\n",
    "#     lossdf.drop(LOSSFIELD, axis=1, inplace=True)\n",
    "# lossdf[1] = 0\n",
    "\n",
    "# #Calculate the average annual loss\n",
    "# aeps = probability(np.array(lossdf.columns.to_list()))\n",
    "# lossdf['AAL'] = (lossdf).apply(simpson, axis=1, x=-1*aeps)\n",
    "# lossdf.to_csv(pjoin(OUTPATH, state_name, f\"{LOSSFIELD}_SA1.csv\"))\n",
    "# ArcGIS_table_loc = pjoin(OUTPATH, state_name, f\"{LOSSFIELD}_SA1.csv\")\n",
    "# #Save AEP calculations.\n",
    "# aeps_save = pd.DataFrame(aeps)\n",
    "# aeps_save.to_csv(pjoin(OUTPATH, state_name, f\"{LOSSFIELD}_aeps.csv\"))\n",
    "# #Calculate and save loss stats.\n",
    "# lossdf_stats = AAL_loss_stats2(lossdf,state_name,LOSSFIELD,OUTPATH,state_name)\n",
    "\n",
    "# #Plot histogram of AAL (structural loss ratio) counts for the entire region (not breaking down by LGA yet).\n",
    "# #SA1 agg\n",
    "# fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# sns.histplot(lossdf['AAL'], ax=ax)\n",
    "# ax.set_yscale('log')\n",
    "# plt.xlim(left=0)\n",
    "# ax.set_xlabel('AAL')\n",
    "# ax.set_ylabel('Count')\n",
    "# ax.set_title('SA1 aggregate')\n",
    "# plt.savefig(pjoin(OUTPATH, state_name,\n",
    "#                 f\"{LOSSFIELD}_SA1\"))\n",
    "\n",
    "# #Groupby LGA code to plot histogram as LGA agg (rather than SA1 agg which is what is used by default).\n",
    "# lossdf_LGA = lossdf.join(df)\n",
    "# lossdf_LGA = lossdf_LGA.drop(['LGA_NAME','AAL'], axis=1)\n",
    "# fields = ['LGA_CODE']\n",
    "# lossdf_LGA = lossdf_LGA.groupby(fields).\\\n",
    "#     mean()\n",
    "# lossdf_LGA['AAL'] = (lossdf_LGA).apply(simpson, axis=1, x=-1*aeps)\n",
    "# lossdf_LGA.to_csv(pjoin(OUTPATH, state_name, f\"{LOSSFIELD}_LGA.csv\"))\n",
    "\n",
    "# #Plot histogram of AAL (structural loss ratio) counts for the entire region.\n",
    "# #LGA agg\n",
    "# fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# sns.histplot(lossdf_LGA['AAL'], ax=ax)\n",
    "# plt.xlim(left=0)\n",
    "# ax.set_xlabel('AAL')\n",
    "# ax.set_ylabel('Count')\n",
    "# ax.set_title('LGA aggregate')\n",
    "# plt.savefig(pjoin(OUTPATH, state_name,\n",
    "#                 f\"{LOSSFIELD}_LGA\"))\n",
    "\n",
    "# #Break the data down by LGA.\n",
    "# #Create stats.csv, count plot and prob plot for each LGA of interest.\n",
    "# for index, LGA_code in LGAs.iterrows():\n",
    "#     LGAdf = df.loc[df['LGA_CODE'] == LGA_code['LGA_CODE']]\n",
    "#     LGAname = LGA_code['LGA_NAME']\n",
    "#     LGAname = LGAname[:-4]\n",
    "#     LGAname = LGAname.replace(\" \", \"_\")\n",
    "#     lossdf4 = lossdf.merge(LGAdf, left_index=True, right_index=True)\n",
    "#     lossdf4.columns = lossdf4.columns.map(str)\n",
    "#     lossdf4 = lossdf4.rename(columns={c: 'RP' + c for c in lossdf4.columns if c not in ['SA1_CODE', 'AAL', 'LGA_CODE', 'LGA_NAME']})\n",
    "#     lossdf4.to_csv(pjoin(OUTPATH, state_name, f\"{LOSSFIELD}_{LGAname}_SA1.csv\"))\n",
    "#     AAL_loss_stats2(lossdf4, LGAname, LOSSFIELD, OUTPATH, state_name)\n",
    "#     plotaal_count_ratio(lossdf4, LGAname, LOSSFIELD, OUTPATH, state_name)\n",
    "#     lossdf4 = lossdf4.drop(['AAL','LGA_CODE','LGA_NAME'], axis=1)\n",
    "#     lossdf4 = lossdf4.mean(axis=0)\n",
    "#     plotaal_prob_ratio(lossdf4, LGAname, LOSSFIELD, OUTPATH, state_name)\n",
    "\n",
    "# file_format3 = os.path.join(OUTPATH, state_name, \"{}_{}_SA1.csv\") #General format of aggregated HazImp output.\n",
    "\n",
    "# #Find mean of structural loss ratio mean to create AAL prob plot.\n",
    "# #SA1 agg\n",
    "# lossdf = lossdf.mean(axis=0)\n",
    "# lossdf = lossdf.drop(['AAL'])\n",
    "\n",
    "# #Create AAL prob plot for entire dataset.\n",
    "# #SA1 agg\n",
    "# fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# sns.lineplot(x=lossdf, y=aeps, ax=ax)\n",
    "# ax.set_yscale('log')\n",
    "# plt.xlim([0,0.75])\n",
    "# plt.ylim(bottom=0.0001)\n",
    "# ax.set_xlabel('Structural Loss Ratio')\n",
    "# ax.set_ylabel('Annual Probability')\n",
    "# ax.set_title('SA1 aggregate')\n",
    "# ax.fill_between(lossdf, aeps, alpha=0.2)\n",
    "# plt.savefig(pjoin(OUTPATH, state_name,\n",
    "#                 f\"{LOSSFIELD}_SA1_probability_loss\"))\n",
    "\n",
    "# #Find mean of structural loss ratio mean to create AAL prob plot.\n",
    "# #LGA agg\n",
    "# lossdf_LGA = lossdf_LGA.mean(axis=0)\n",
    "# lossdf_LGA = lossdf_LGA.drop(['AAL'])\n",
    "\n",
    "# #Create AAL prob plot for entire dataset.\n",
    "# #LGA agg\n",
    "# fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# sns.lineplot(x=lossdf_LGA, y=aeps, ax=ax)\n",
    "# ax.set_yscale('log')\n",
    "# plt.xlim([0,0.75])\n",
    "# plt.ylim(bottom=0.0001)\n",
    "# ax.set_xlabel('Structural Loss Ratio')\n",
    "# ax.set_ylabel('Annual Probability')\n",
    "# ax.set_title('LGA aggregate')\n",
    "# ax.fill_between(lossdf_LGA, aeps, alpha=0.2)\n",
    "# plt.savefig(pjoin(OUTPATH, state_name,\n",
    "#                 f\"{LOSSFIELD}_LGA_probability_loss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section imports AAL tables into ArcGIS Pro and spatialises them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is formatted to work for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define metadata.\n",
    "#Metadata text can be defined here. {} indicates that a variable will be used in these locations.\n",
    "#Variables are defined further down.\n",
    "#If you would like to change the variables or add variables you will need to edit the .format(variable) function.\n",
    "\n",
    "#Aggregated table metadata.\n",
    "agg_table_title = '{}' # name of table (agg_table_name)\n",
    "agg_table_tags = 'NWRA, ACS, risk, impact'\n",
    "agg_table_summary = 'Raw tabulated {} SA1 aggregated average annual loss (AAL) values.' # state name\n",
    "agg_table_description = 'Raw, tabulated HazImp calculations of average structural loss ratio for all recurrence intervals, \\\n",
    "and average annual loss (AAL) for {}, aggregated to SA1 code (2021). Raw HazImp data location {}' # state name, location of input table in directory\n",
    "agg_table_credits = 'Commonwealth of Australia (Geoscience Australia)'\n",
    "agg_table_accessConstraints = 'Creative Commons Attribution 4.0 International Licence. \\\n",
    "<a href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</a>'\n",
    "\n",
    "#Aggregated feature class metadata.\n",
    "agg_feature_title = '{}' # name of feature class\n",
    "agg_feature_tags = agg_table_tags\n",
    "agg_feature_summary = '{} SA1 aggregated average annual loss (AAL) values.' # state name\n",
    "agg_feature_description = 'HazImp calculations of average annual loss (AAL) in {}, aggregated to SA1 code (2021). Spatialised \\\n",
    "using raw HazImp output from {}' # state name, recurrence interval years, location of input table in directory\n",
    "agg_feature_credits = agg_table_credits\n",
    "agg_feature_accessConstraints = agg_table_accessConstraints\n",
    "\n",
    "#Define names for ArcGIS output.\n",
    "agg_table_name = LOSSFIELD + '_' + state_name + '_AAL_table'\n",
    "agg_feature_name = LOSSFIELD + '_' + state_name + '_AAL'\n",
    "\n",
    "#Import table into ArcGIS.\n",
    "arcpy.conversion.ExportTable(in_table = ArcGIS_table_loc,\n",
    "                             out_table = os.path.join(table_out_path, agg_table_name))\n",
    "\n",
    "#Create copy of SA1 boundaries.\n",
    "arcpy.conversion.ExportFeatures(in_features = agg_boundary_loc,\n",
    "                                out_features = os.path.join(feature_out_path, agg_feature_name))\n",
    "\n",
    "#Add a field to the hazImp agg table and assign the SA1 CODE values. \n",
    "#(SA1 in the hazImp output is a double but it is text in the boundaries feature class, they need to be the same in order for the join to work).\n",
    "arcpy.management.AddField(in_table = os.path.join(table_out_path, agg_table_name),\n",
    "                          field_name = 'SA1_CODE21',\n",
    "                          field_type = 'TEXT')\n",
    "arcpy.management.CalculateField(in_table = os.path.join(table_out_path, agg_table_name),\n",
    "                                field = 'SA1_CODE21',\n",
    "                                expression = '!SA1_CODE!',\n",
    "                                expression_type = 'PYTHON3')\n",
    "\n",
    "#Join the aggregated hazImp output table with the SA1 boundaries.\n",
    "arcpy.management.JoinField(in_data = os.path.join(feature_out_path, agg_feature_name),\n",
    "                           in_field = 'SA1_CODE21',\n",
    "                           join_table = os.path.join(table_out_path, agg_table_name),\n",
    "                           join_field = 'SA1_CODE21')\n",
    "\n",
    "#The copy of the SA1 boundaries we used is a national dataset but the data we are joining is not national. \n",
    "#Use update cursor to delete rows (SA1s) which have null values.\n",
    "#This will leave only the SA1s in the state which have residential building (and loss) data.\n",
    "with arcpy.da.UpdateCursor(os.path.join(feature_out_path, agg_feature_name), 'AAL', 'AAL is NULL') as cursor:\n",
    "    for row in cursor:\n",
    "            cursor.deleteRow()\n",
    "\n",
    "#Create a metadata file and add predefined text for the agg table\n",
    "agg_table_md =  md.Metadata()\n",
    "agg_table_md.title = agg_table_title.format(agg_table_name)\n",
    "agg_table_md.tags = agg_table_tags\n",
    "agg_table_md.summary = agg_table_summary.format(state_name)\n",
    "agg_table_md.description = agg_table_description.format(state_name, ArcGIS_table_loc)\n",
    "agg_table_md.credits = agg_table_credits\n",
    "agg_table_md.accessConstraints = agg_table_accessConstraints\n",
    "\n",
    "#Select the agg table metadata and overwrite it with the predefined metadata\n",
    "target_md = md.Metadata(os.path.join(table_out_path, agg_table_name))\n",
    "if not target_md.isReadOnly:\n",
    "       target_md.copy(agg_table_md)\n",
    "       target_md.save()\n",
    "            \n",
    "#Create a metadata file and add predefined text for the agg feature class\n",
    "agg_feature_md = md.Metadata()\n",
    "agg_feature_md.title = agg_feature_title.format(agg_feature_name)\n",
    "agg_feature_md.tags = agg_feature_tags\n",
    "agg_feature_md.summary = agg_feature_summary.format(state_name)\n",
    "agg_feature_md.description = agg_feature_description.format(state_name, ArcGIS_table_loc)\n",
    "agg_feature_md.credits = agg_feature_credits\n",
    "agg_feature_md.accessConstraints = agg_feature_accessConstraints\n",
    "\n",
    "#Select the agg feature class metadata and overwrite it with the predefined metadata\n",
    "target_md = md.Metadata(os.path.join(feature_out_path, agg_feature_name))\n",
    "if not target_md.isReadOnly:\n",
    "       target_md.copy(agg_feature_md)\n",
    "       target_md.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " this section is formatted to create sepereate feature classes for different regions (e.g. LGAs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Define metadata.\n",
    "# #Metadata text can be defined here. {} indicates that a variable will be used in these locations.\n",
    "# #Variables are defined further down.\n",
    "# #If you would like to change the variables or add variables you will need to edit the .format(variable) function.\n",
    "\n",
    "# #Aggregated table metadata.\n",
    "# agg_table_title = '{}' # name of table (agg_table_name)\n",
    "# agg_table_tags = 'NWRA, ACS, risk, impact'\n",
    "# agg_table_summary = 'Raw tabulated {} SA1 aggregated average annual loss (AAL) values.' # LGA name\n",
    "# agg_table_description = 'Raw, tabulated HazImp calculations of average structural loss ratio for all recurrence intervals, \\\n",
    "# and average annual loss (AAL) for {}, aggregated to SA1 code (2021). Raw HazImp data location {}' # LGA name, location of input table in directory\n",
    "# agg_table_credits = 'Commonwealth of Australia (Geoscience Australia)'\n",
    "# agg_table_accessConstraints = 'Creative Commons Attribution 4.0 International Licence. \\\n",
    "# <a href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</a>'\n",
    "\n",
    "# #Aggregated feature class metadata.\n",
    "# agg_feature_title = '{}' # name of feature class\n",
    "# agg_feature_tags = agg_table_tags\n",
    "# agg_feature_summary = '{} SA1 aggregated average annual loss (AAL) values.' # state name\n",
    "# agg_feature_description = 'HazImp calculations of average annual loss (AAL) in {}, aggregated to SA1 code (2021). Spatialised \\\n",
    "# using raw HazImp output from {}' # state name, recurrence interval years, location of input table in directory\n",
    "# agg_feature_credits = agg_table_credits\n",
    "# agg_feature_accessConstraints = agg_table_accessConstraints\n",
    "\n",
    "# for LGA in LGAs_6:\n",
    "#     LGAname = LGA\n",
    "#     LGAname = LGAname[:-4]\n",
    "#     LGA_name = LGAname\n",
    "#     LGAname = LGAname.replace(\" \", \"_\")\n",
    "#     #Define names for ArcGIS output\n",
    "#     agg_table_name = LOSSFIELD + '_' + LGAname + '_AAL_table'\n",
    "#     agg_feature_name = LOSSFIELD + '_' + LGAname + '_AAL'\n",
    "#     ArcGIS_table_loc = file_format3.format(LOSSFIELD, LGAname)\n",
    "#     #Import table into ArcGIS\n",
    "#     arcpy.conversion.ExportTable(in_table = ArcGIS_table_loc,\n",
    "#                                  out_table = os.path.join(table_out_path, agg_table_name))\n",
    "#     #Create copy of SA1 boundaries\n",
    "#     arcpy.conversion.ExportFeatures(in_features = agg_boundary_loc,\n",
    "#                                     out_features = os.path.join(feature_out_path, agg_feature_name))\n",
    "\n",
    "#     #Add a field to the hazimp agg table and assign the SA1 CODE values.\n",
    "#     #(SA1 in the hazimp output is a double but it is text in the boundaries feature class, they need to be the same in order for the join to work).\n",
    "#     arcpy.management.AddField(in_table = os.path.join(table_out_path, agg_table_name),\n",
    "#                               field_name = 'SA1_CODE21',\n",
    "#                               field_type = 'TEXT')\n",
    "#     arcpy.management.CalculateField(in_table = os.path.join(table_out_path, agg_table_name),\n",
    "#                                     field = 'SA1_CODE21',\n",
    "#                                     expression = '!SA1_CODE!',\n",
    "#                                     expression_type = 'PYTHON3')\n",
    "\n",
    "#     #Join the aggregated hazimp output table with the SA1 boundaries.\n",
    "#     arcpy.management.JoinField(in_data = os.path.join(feature_out_path, agg_feature_name),\n",
    "#                                in_field = 'SA1_CODE21',\n",
    "#                                join_table = os.path.join(table_out_path, agg_table_name),\n",
    "#                                join_field = 'SA1_CODE21')\n",
    "\n",
    "#     #The copy of the SA1 boundaries we used is a national dataset but the data we are joining is not national.\n",
    "#     #Use update cursor to delete rows (SA1s) which have null values.\n",
    "#     #This will leave only the SA1s in the state which have residential building (and loss) data.\n",
    "#     with arcpy.da.UpdateCursor(os.path.join(feature_out_path, agg_feature_name), 'AAL', 'AAL is NULL') as cursor:\n",
    "#         for row in cursor:\n",
    "#                 cursor.deleteRow()\n",
    "\n",
    "#     #Create a metadata file and add predefined text for the agg table.\n",
    "#     agg_table_md =  md.Metadata()\n",
    "#     agg_table_md.title = agg_table_title.format(agg_table_name)\n",
    "#     agg_table_md.tags = agg_table_tags\n",
    "#     agg_table_md.summary = agg_table_summary.format(LGA_name)\n",
    "#     agg_table_md.description = agg_table_description.format(LGA_name, ArcGIS_table_loc)\n",
    "#     agg_table_md.credits = agg_table_credits\n",
    "#     agg_table_md.accessConstraints = agg_table_accessConstraints\n",
    "\n",
    "#     #Select the agg table metadata and overwrite it with the predefined metadata.\n",
    "#     target_md = md.Metadata(os.path.join(table_out_path, agg_table_name))\n",
    "#     if not target_md.isReadOnly:\n",
    "#            target_md.copy(agg_table_md)\n",
    "#            target_md.save()\n",
    "            \n",
    "#     #Create a metadata file and add predefined text for the agg feature class.\n",
    "#     agg_feature_md = md.Metadata()\n",
    "#     agg_feature_md.title = agg_feature_title.format(agg_feature_name)\n",
    "#     agg_feature_md.tags = agg_feature_tags\n",
    "#     agg_feature_md.summary = agg_feature_summary.format(LGA_name)\n",
    "#     agg_feature_md.description = agg_feature_description.format(LGA_name, ArcGIS_table_loc)\n",
    "#     agg_feature_md.credits = agg_feature_credits\n",
    "#     agg_feature_md.accessConstraints = agg_feature_accessConstraints\n",
    "\n",
    "#     #Select the agg feature class metadata and overwrite it with the predefined metadata.\n",
    "#     target_md = md.Metadata(os.path.join(feature_out_path, agg_feature_name))\n",
    "#     if not target_md.isReadOnly:\n",
    "#            target_md.copy(agg_feature_md)\n",
    "#            target_md.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('hazimp3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8e26b18f96b40a85c02df06fa5943cccfe4d0358772ac4d409380609a987ec9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
